\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}


\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    showstringspaces=false,
    frame=single,
    breaklines=true
}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{WhatsApp Chat Summarization}

\author{Andrea Auletta\\
{\tt\small andrea.auletta@studenti.unipd.it}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Davide Baggio\\
{\tt\small davide.baggio.1@studenti.unipd.it}
\and
Marco Bernardi\\
{\tt\small marco.bernardi.11@studenti.unipd.it}
\and
Marco Brigo\\
{\tt\small marco.brigo@studenti.unipd.it}
\and
Sebastiano Sanson\\
{\tt\small sebastiano.sanson@studenti.unipd.it}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}This research project in Natural Language Processing (NLP) 
   focuses on the development of an automated system for the summarization 
   of chat messages, applicable to both group and individual conversations. The objective is to generate concise summaries of conversations, 
   thereby obviating the need for users to review all messages individually. 
   This application is especially pertinent in contexts where chat logs proliferate quickly, 
   offering significant benefits in terms of time savings and cognitive load reduction for users.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

In the era of digital communication, chat platforms have become fundamentals in our daily interactions, whether they are personal or professional. This new type of transmission favors real-time messages exchange which allows users to engage in conversations that can quickly accumulate extensive chat logs. The huge volume of messages, especially in active group chats formed by dozens if not hundreds of users, often makes it difficult for users to stay updated without dedicating substantial time to review each message. This challenge highlights the need for an efficient method that enables users to quickly extract essential information from chat conversations.

This research project addresses this need by developing an automated system for summarizing chat messages with the goal to generate concise summaries, reducing the need to sort through all messages. By doing so, users can quickly obtain the main points of discussions, significantly saving time and reducing cognitive load.

The significance of this application is evident in various contexts: for instance, in corporate environments, project teams often heavily rely on group chats to coordinate tasks, share updates and exchange miscellaneous informations; similarly, social groups frequently engage in dynamic conversations. In both scenarios, an automated summarization system can enhance productivity and ensure important information is not overlooked.

This research aims to explore and implement state of the art NLP techniques to create an effective summarization system, focusing on the datasets used, the manipulation of them in order to obtain always a coherent data structure, how we obtained new examples to test the performances and the results we got after all.

TO DO: overview of our main results!

\section{Datasets}

In this section, we explore datasets essential for training and evaluating automated summarization systems for chat messages: an ideal dataset should have a variety of conversation styles and contexts to ensure the robustness and versatility of the summarization models. The datasets should contain real-life or realistic dialogues annotated with concise and accurate summaries. Additionally, the data should reflect diverse communication patterns, including informal and formal registers, and incorporate common conversational elements such as slang, emojis, eventual typos, the use of voice messages and images. We present below two notable datasets, SAMSum and DialogSum (both obtained by Hugging Face repository), which fulfill these criteria and provide rich resources for advancing the field of conversational AI.

\subsection{SAMSum}

The SAMSum~\cite{DBLP:journals/corr/abs-1911-12237} dataset contains about 16,000 messenger-like conversations with summaries splitted in:
\begin{itemize}
    \item training: 14,732;
    \item validation: 818;
    \item test: 819.
\end{itemize}
Conversations were created and written down by linguists fluent in English: they were asked to create them similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger conversations. 
The style and register are diversified - conversations could be informal, semi-formal or formal, 
they may contain slang words, emoticons and typos. Then, the conversations were annotated with summaries. 
It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person. 
The SAMSum dataset was prepared by Samsung R\&D Institute Poland and is distributed for research purposes.

% dialogsum tolta una colonna topic
\subsection{Validation Dataset}

To rigorously assess the performance of the selected models within our specific use case, 
we constructed a testing dataset by generating new examples derived from authentic WhatsApp conversations.

\subsubsection{Data Collection}
WhatsApp provides a robust functionality for exporting chat conversations in a .txt file format. 
This exported file encompasses all exchanged messages, complete with timestamps and sender names. 
Additionally, the file includes various media types, such as images, voice messages, and videos. 
This comprehensive dataset is instrumental for rigorous evaluation as it reflects real-world conditions and the diverse communication forms encountered in practical applications.

\subsubsection{Data Preprocessing}

The exported .txt file underwent a meticulous preprocessing phase to extract the pertinent information, including message content and sender details. 
This information was structured into a Python dictionary as follows:

\begin{lstlisting}
    dialogues = [
        {
            'text': "Hello, how are you?",
            'id': 0,
            'golden_summary': "A greeting"
        }
    ]
\end{lstlisting}
For conversations containing images or voice messages, the filenames of these media were embedded within the text field to facilitate subsequent processing by replacing the filenames with detailed descriptions for images and transcriptions for voice messages. 
The models utilized for generating these descriptions and transcriptions are discussed in the following sections.
We also scraped out irrelevant information, like the date-time, in order to match the same structure used in the training dataset mentioned in the previous sections 

This preprocessing approach ensures that the dataset is both comprehensive and structured, enabling effective analysis and model evaluation.
\section{Models Involved}

\subsection{BART}

BART~\cite{lewis2019bart} is a denoising autoencoder that maps a corrupted document to the original document it was derived from. It is implemented as a sequence-to-sequence model with a bidirectional encoder over 
corrupted text and a left-to-right autoregressive decoder. \\
In the architecture there are 6 layers both in the encoder and decoder (while the large model uses 12 layers in each). Each layer of the decoder performs cross-attention over the final hidden layer of the 
encoder. This pretraining enables BART to effectively learn robust representations of text.\\
BART is trained by corrupting documents and then optimizing a reconstruction loss between the decoder's output and the original document. \\
The part that interests us is the fine-tuning one, this model can be used in several ways for downstream applications like: sequence classification, token classification, sequence generation
and machine translation tasks. 

\subsection{FLAN-T5}

FLAN-T5~\cite{chung2022scalinginstructionfinetunedlanguagemodels} is an enhanced version of the T5 model (Text-To-Text Transfer Transformer) that has been fine-tuned on more than 1,000 additional tasks, aiming to improve the performance on natural language understanding and generation tasks. It comes in various sizes, from 80 million parameters (Flan-T5-Small) to 11 billion parameters (Flan-T5-XXL). Due to our limited computational resources, we have chosen the base model, which includes 248M parameters.\\
The T5 model follows the Transformer architecture’s encoder-decoder structure, which is crucial for handling text-to-text tasks and is characterized by:
\begin{itemize}
    \item Encoder: processes the input embeddings and generates a sequence of hidden states. Each layer of it consists of a multi-head self-attention mechanism followed by a feed-forward neural network. The former allows the model to focus on different parts of the input sequence when encoding each token, while the latter consists of two linear transformations with a ReLU activation in between;
    \item Decoder: takes these hidden states and generates the output text, one token at a time, using autoregressive generation. It's similar to the encoder, but it includes also an additional layer of attention over the encoder’s outputs to ensure that the prediction for a particular token can only depend on the known outputs before it.
\end{itemize}
Finally, the output is generated by a softmax function that converts the decoder's hidden states into probabilities reflected on the vocabulary in natural language. \\ \\
The comparison between T5 and FLAN-T5, both in the base version, highlights how the fine-tuned one outperforms the former model in every relevant benchmark for our scope: 
\begin{itemize}
    \item \textbf{MMLU} (Massive Multitask Language Understanding): it evaluates the performance across a wide range of tasks, subjects and disciplines, assessing the ability to understand and generate human language by testing them on various questions.
    \begin{itemize}
        \item Direct propmpting: 25.7 - 35.9
        \item CoT: 14.5 - 33.7
    \end{itemize}
    \item \textbf{BBH} (Big-Bench Hard): is a collection of challenging tasks designed to evaluate the performance on difficult, nuanced, and often high-level natural language understanding tasks. 
    \begin{itemize}
        \item Direct propmpting: 27.8 - 31.3
        \item CoT: 14.6 - 27.9
    \end{itemize}
    \item \textbf{TyDiQA} (Typologically Diverse Question Answering): it evaluates the performance of question-answering systems across a diverse set of languages.
    \begin{itemize}
        \item Direct prompting: 0.0 - 4.1
    \end{itemize}
\end{itemize}

\subsection{Multimedia models}

Because chat messages often include multimedia contents, we have decided to use specific models to convert this type of data into text that can be summarized by the previously cited model. In particular, we transform images and voice messages, which are very frequently used nowadays. 
Since our project regards text chat summarization, we will not focus into the technical details of how the following models work in this paper. This approach ensures that our main objective is clearly addressed without overcomplicating the discussion with the intricate workings of the underlying models.

\subsubsection{Florence-2 Large}

To ensure that the images were processed by the summarization model, it was necessary to convert them into detailed textual descriptions. 
For this task, we employed Florence-2 by Microsoft~\cite{xiao2023florence2advancingunifiedrepresentation}, an advanced vision foundation model that utilizes a prompt-based approach to address a wide array of vision and vision-language tasks. 
Florence-2 is designed to interpret text prompts as task instructions and produce the desired textual outputs, including captioning, object detection, grounding, and segmentation. 
This model is built upon the extensive FLD-5B dataset, which comprises 5.4 billion annotations across 126 million images, thereby enhancing its multi-task learning capabilities. 
The sequence-to-sequence architecture of Florence-2 allows it to perform effectively in both zero-shot and fine-tuned scenarios, establishing itself as a robust and competitive vision foundation model.

\subsubsection{Whisper}

In addition to processing images, it was necessary to convert voice messages to text. 
For this task, we utilized Whisper by OpenAI~\cite{radford2022robustspeechrecognitionlargescale}, an automatic speech recognition (ASR) system. 
Whisper is trained on 680,000 hours of multilingual and multitask supervised data collected from the web. 
The extensive and diverse dataset enhances the system's robustness to accents, background noise, and technical language, which is particularly beneficial for our use case, as voice messages are often of suboptimal quality. 
Furthermore, Whisper supports transcription in multiple languages and translation from those languages into English.

\section{Fine Tuning}

\section{Testing}

\section{Results}

\subsection{Metrics}
We evaluated the results provided by different NLP models using the ROUGE set of metrics, which stands for Recall-Oriented Understudy for Gisting Evaluation. We chose this specific metric because it is currently the most effective way to assess the performance of automatic summarization systems. It compares the generated summary with one or more human made references by measuring the overlap of n-grams in them. \\ \\ 
In particular, we have used the following metrics:
\begin{itemize}
    \item \textbf{ROUGE-1}: as the number suggests, it evaluates the overlap of unigrams, meaning that it compares the single words between the summaries;
    \item \textbf{ROUGE-2}: this time it evaluates the overlap of bigrams, that are two consecutive words;
    \item \textbf{ROUGE-L}: this one instead evaluates the longest common subsequence, meaning that it considers the structure of the sentences and the word order, but not requiring the words to be consecutive;
    \item \textbf{ROUGE-LSum}: it is related to the previous one, but it uses a slightly different calculation method since it applies the ROUGE-L's method at the sentence level and then aggregates all the results for the final score. This metric is seen as more suitable for tasks where sentence level extraction is valuable such as extractive summarization tasks.
\end{itemize}
Each metric is calculated as follows:
\begin{itemize}
    \item \textbf{Precision}: it measures the accuracy of the positive predictions made by the model. It is the ratio of correctly predicted positive instances to the total predicted positive instances \\
    \[ \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}} \]
    \item \textbf{Recall}: it measures the ability of the model to find all relevant positive instances in the dataset. It is the ratio of correctly predicted positive instances to the total actual positive instances. \\
    \[ \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}} \]
    \item \textbf{F1-Score}: is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall, which is useful when the dataset has an uneven class distribution or when the cost of false positives and false negatives is different. \\
    \[ \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \]
\end{itemize}
The value returned for each ROUGE metric is the F1-Score, since it provides a balanced measure of a model's performance by combining both precision and recall. By doing so, it avoids scenarios where a model might have high precision but low recall values, or vice versa. This is especially important when both false positives and false negatives carry significant costs.

\subsection{Human evaluation}
We evaluated the results of the model on the WhatsApp chats in this way: given the 12 chats we had, each member of the group voted if the summary was good or not.
The final results were obtained by the successive formulas:
\begin{itemize}
    \item Proportion of annotations for which all the members agree:
    \begin{equation}
        P_o = \frac{\sum_i \text{count}(A=B=C=D=E=y_i)}{\text{total annotations}}
    \end{equation}
    where A, B, C, D, E represent the members of the group, and $y_i$ the label (can be 0 or 1);
    \item Agreement expected by chance:
    \begin{equation}
        P_e = \sum_i P(A=y_i) \ldots P(E=y_i)
    \end{equation}
    \item Cohen's Kappa:
    \begin{equation}
        K = \frac{P_o - P_e}{1 - P_e}
    \end{equation}
\end{itemize}
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    Po & Pe & K \\ 
    \hline
    1 & 1 & 1 \\
    \hline
\end{tabular}
\caption{Results of the human evaluation}
\label{table:example}
\end{table}



%\subsection{Suggested Structure}

%The following is a suggested structure for your report:

%\begin{itemize}
%	\item Introduction (20\%): describe the problem you are working on, why it's important, what are your goals, and provide also an overview of your main results.
%	\item Dataset (20\%): describe the data you are working with for your project. What type of data is it? Where did it come from? How much data are you working with? Did you have to do any preprocessing, filtering, etc., and why?
%	\item Method (30\%): discuss your approach for solving the problems that you set up in the introduction. Why is your approach the right thing to do? Did you consider alternative approaches? It may be helpful to include figures, diagrams, or tables to describe your method or compare it with others.
%	\item Experiments (30\%): discuss the experiments that you performed. The exact experiments will vary depending on the project, but you might compare with prior work, perform an ablation study to determine the impact of various components of your system, experiment with different hyperparameters or architectural choices. You should include graphs, tables, or other figures to illustrate your experimental results.
%\end{itemize}	

%-------------------------------------------------------------------------
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
